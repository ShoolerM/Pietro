"""Controller for LLM operations and streaming."""

import threading
import hashlib
import warnings
from PyQt5 import QtCore
from langchain_openai import ChatOpenAI
from langchain_core.callbacks import StreamingStdOutCallbackHandler
from langchain_core.messages import SystemMessage, HumanMessage
from models.story_model import StoryModel


class StreamingSignals(QtCore.QObject):
    """QObject to hold Qt signals for thread-safe communication."""

    text_signal = QtCore.pyqtSignal(str)
    thinking_signal = QtCore.pyqtSignal(str)
    render_markdown_signal = QtCore.pyqtSignal()
    set_waiting_signal = QtCore.pyqtSignal(bool)
    set_stop_enabled_signal = QtCore.pyqtSignal(bool)


class SummarizationSignals(QtCore.QObject):
    """QObject to hold Qt signals for summarization progress."""

    thinking_signal = QtCore.pyqtSignal(str)
    progress_signal = QtCore.pyqtSignal(int, int)  # current, total
    completed_signal = QtCore.pyqtSignal(str, int)  # story_for_llm, tokens
    error_signal = QtCore.pyqtSignal(str)  # error_message
    set_waiting_signal = QtCore.pyqtSignal(bool)


class QtStreamingCallbackHandler(StreamingStdOutCallbackHandler):
    """Custom callback handler that streams LLM output to a Qt signal.
    Filters out thinking blocks between <think> and </think> tags.
    Thinking content is sent to a separate signal.
    """

    def __init__(self, signals, stop_check=None, paragraph_limit=None):
        super().__init__()
        self.signals = signals
        self.stop_check = stop_check
        self.buffer = ""
        self.in_thinking_block = False
        self.paragraph_limit = paragraph_limit
        self.paragraph_count = 0
        self.accumulated_text = ""

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        """Called when a new token is generated by the LLM.
        Filters out content between <think> and </think> tags.
        """
        # Check for stop request - MUST be checked first
        if self.stop_check:
            try:
                is_stopped = self.stop_check()
                if is_stopped:
                    # Signal to stop and raise immediately
                    self.signals.thinking_signal.emit(
                        "\n[Stop requested - will interrupt LLM]\n"
                    )
                    raise KeyboardInterrupt("Generation stopped by user")
            except KeyboardInterrupt:
                raise
            except Exception:
                pass

        self.buffer += token

        if "<think>" in self.buffer:
            before_think = self.buffer.split("<think>")[0]
            if before_think and not self.in_thinking_block:
                self.signals.text_signal.emit(before_think)
                if self.paragraph_limit and not self.in_thinking_block:
                    self.accumulated_text += before_think
                    self._check_paragraph_limit()
            self.in_thinking_block = True
            self.buffer = (
                self.buffer.split("<think>", 1)[1] if "<think>" in self.buffer else ""
            )

        if "</think>" in self.buffer and self.in_thinking_block:
            thinking_content = self.buffer.split("</think>", 1)[0]
            if thinking_content:
                self.signals.thinking_signal.emit(thinking_content)
            after_think = (
                self.buffer.split("</think>", 1)[1] if "</think>" in self.buffer else ""
            )
            self.in_thinking_block = False
            self.buffer = after_think
            if self.buffer and not self.in_thinking_block:
                return

        if self.in_thinking_block and len(self.buffer) > 10:
            to_emit = self.buffer[:-10]
            self.buffer = self.buffer[-10:]
            if to_emit:
                self.signals.thinking_signal.emit(to_emit)
        elif not self.in_thinking_block and len(self.buffer) > 10:
            to_emit = self.buffer[:-10]
            self.buffer = self.buffer[-10:]
            if to_emit:
                self.signals.text_signal.emit(to_emit)
                if self.paragraph_limit:
                    self.accumulated_text += to_emit
                    self._check_paragraph_limit()

        # Extra stop check after each token processing
        if self.stop_check and self.stop_check():
            raise KeyboardInterrupt("Generation stopped by user")

    def _check_paragraph_limit(self):
        """Check if we've reached the paragraph limit and stop if so."""
        if not self.paragraph_limit:
            return

        # Also check for stop request while checking paragraph limit
        if self.stop_check and self.stop_check():
            raise KeyboardInterrupt("Generation stopped by user")

        # Count paragraphs (double newlines)
        self.paragraph_count = self.accumulated_text.count("\n\n")

        if self.paragraph_count >= self.paragraph_limit:
            raise KeyboardInterrupt(
                f"Paragraph limit of {self.paragraph_limit} reached"
            )

    def on_llm_end(self, *args, **kwargs) -> None:
        """Called when LLM finishes. Emit any remaining buffer."""
        if self.in_thinking_block and self.buffer:
            self.signals.thinking_signal.emit(self.buffer)
        elif not self.in_thinking_block and self.buffer:
            self.signals.text_signal.emit(self.buffer)
            if self.paragraph_limit:
                self.accumulated_text += self.buffer
        self.buffer = ""
        self.in_thinking_block = False


class LLMController:
    """Handles LLM operations and streaming."""

    def __init__(self, llm_model, story_model, settings_model):
        """Initialize the LLM controller.

        Args:
            llm_model: LLMModel instance
            story_model: StoryModel instance
            settings_model: SettingsModel instance
        """
        self.llm_model = llm_model
        self.story_model = story_model
        self.settings_model = settings_model

        # Create LLM instance
        self.llm = ChatOpenAI(
            base_url=self.llm_model.base_url,
            streaming=True,
            temperature=self.llm_model.temperature,
        )

        # Session cache for chunked story to avoid re-chunking on every summarization
        self._cached_chunks = None
        self._cached_chunks_hash = None

    def update_model(self, model_name):
        """Update the LLM model.

        Args:
            model_name: Name of the model to use
        """
        if (
            model_name
            and not model_name.startswith("Error:")
            and model_name != "No models available"
        ):
            try:
                self.llm = ChatOpenAI(
                    base_url=self.llm_model.base_url,
                    model=model_name,
                    streaming=True,
                    temperature=self.llm_model.temperature,
                )
                self.llm_model.current_model = model_name
            except Exception as e:
                print(f"Failed to update model: {e}")

    def invoke_llm(
        self,
        query,
        system_prompt,
        append_text_callback,
        append_thinking_callback,
        render_markdown_callback,
        set_waiting_callback,
        set_stop_enabled_callback,
    ):
        """Invoke the LLM in a background thread.

        Args:
            query: The query to send to the LLM
            system_prompt: Optional system prompt
            append_text_callback: Callback to append text to view
            append_thinking_callback: Callback to append thinking text to view
            render_markdown_callback: Callback to render markdown in view
            set_waiting_callback: Callback to set waiting state
            set_stop_enabled_callback: Callback to enable/disable stop button
        """
        # Create signals object in main thread
        signals = StreamingSignals()

        # Connect signals with Qt.QueuedConnection to ensure execution on main thread
        signals.text_signal.connect(append_text_callback, QtCore.Qt.QueuedConnection)
        signals.thinking_signal.connect(
            append_thinking_callback, QtCore.Qt.QueuedConnection
        )
        signals.render_markdown_signal.connect(
            render_markdown_callback, QtCore.Qt.QueuedConnection
        )
        signals.set_waiting_signal.connect(
            set_waiting_callback, QtCore.Qt.QueuedConnection
        )
        signals.set_stop_enabled_signal.connect(
            set_stop_enabled_callback, QtCore.Qt.QueuedConnection
        )

        thread = threading.Thread(
            target=self._invoke_thread,
            args=(query, system_prompt, signals),
            daemon=True,
        )
        thread.start()

    def _invoke_thread(self, query, system_prompt, signals):
        """Thread function for LLM invocation.

        Args:
            query: The query to send to the LLM
            system_prompt: Optional system prompt
            signals: StreamingSignals object created in main thread
        """
        try:
            streaming_handler = QtStreamingCallbackHandler(
                signals, lambda: self.llm_model.stop_generation
            )

            if system_prompt:
                messages = [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=query),
                ]
                self.llm.invoke(messages, config={"callbacks": [streaming_handler]})
            else:
                self.llm.invoke(query, config={"callbacks": [streaming_handler]})

            # Check if stop was requested while we were generating
            if self.llm_model.stop_generation:
                signals.text_signal.emit("\n[Generation stopped by user]\n")
                signals.thinking_signal.emit(
                    "\n‚èπÔ∏è Stop requested - generation halted.\n"
                )
            else:
                # Use signal to trigger markdown rendering on UI thread (only if not stopped)
                signals.render_markdown_signal.emit()

        except KeyboardInterrupt as e:
            # User stopped generation - provide feedback
            error_msg = str(e)
            if "stopped by user" in error_msg.lower():
                signals.text_signal.emit("\n[Generation stopped by user]\n")
                signals.thinking_signal.emit(
                    "\n‚èπÔ∏è Stop requested - generation halted.\n"
                )
        except Exception as e:
            # Use signal to append error text on UI thread
            signals.text_signal.emit(f"\n[error] {e}\n")
        finally:
            try:
                # Use signal to hide progress bar on UI thread
                signals.set_waiting_signal.emit(False)
            except Exception:
                pass

    def override_text_with_streaming(
        self,
        query,
        system_prompt,
        stream_callback,
        completion_callback,
        set_stop_enabled_callback,
    ):
        """Override selected text with streaming LLM output.

        Args:
            query: The query with text to rewrite
            system_prompt: Optional system prompt
            stream_callback: Callback for each text chunk (for streaming replacement)
            completion_callback: Callback when generation completes
            set_stop_enabled_callback: Callback to enable/disable stop button
        """

        # Create signals object for thread-safe communication
        class OverrideSignals(QtCore.QObject):
            text_signal = QtCore.pyqtSignal(str)
            completed_signal = QtCore.pyqtSignal()
            set_stop_enabled_signal = QtCore.pyqtSignal(bool)

        signals = OverrideSignals()

        # Connect signals
        signals.text_signal.connect(stream_callback, QtCore.Qt.QueuedConnection)
        signals.completed_signal.connect(
            completion_callback, QtCore.Qt.QueuedConnection
        )
        signals.set_stop_enabled_signal.connect(
            set_stop_enabled_callback, QtCore.Qt.QueuedConnection
        )

        # Start background thread
        thread = threading.Thread(
            target=self._override_thread,
            args=(query, system_prompt, signals),
            daemon=True,
        )
        thread.start()

    def _override_thread(self, query, system_prompt, signals):
        """Thread function for text override with streaming.

        Args:
            query: The query to send to the LLM
            system_prompt: Optional system prompt
            signals: Signal object for thread-safe communication
        """
        try:
            # Create a simple streaming handler that just emits tokens
            class SimpleStreamingHandler(StreamingStdOutCallbackHandler):
                def __init__(self, text_signal, stop_check):
                    super().__init__()
                    self.text_signal = text_signal
                    self.stop_check = stop_check
                    self.buffer = ""

                def on_llm_new_token(self, token: str, **kwargs) -> None:
                    if self.stop_check and self.stop_check():
                        raise KeyboardInterrupt("Generation stopped by user")

                    # Buffer tokens and emit in small batches for smooth streaming
                    self.buffer += token
                    if len(self.buffer) >= 5:  # Emit every 5 characters
                        self.text_signal.emit(self.buffer)
                        self.buffer = ""

                def on_llm_end(self, *args, **kwargs) -> None:
                    # Emit remaining buffer
                    if self.buffer:
                        self.text_signal.emit(self.buffer)
                        self.buffer = ""

            streaming_handler = SimpleStreamingHandler(
                signals.text_signal, lambda: self.llm_model.stop_generation
            )

            # Invoke LLM with streaming
            if system_prompt:
                messages = [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=query),
                ]
                self.llm.invoke(messages, config={"callbacks": [streaming_handler]})
            else:
                self.llm.invoke(query, config={"callbacks": [streaming_handler]})

            # Signal completion
            signals.completed_signal.emit()

        except KeyboardInterrupt:
            # User stopped generation
            pass
        except Exception as e:
            # Emit error as text
            signals.text_signal.emit(f"\n[error] {e}\n")
        finally:
            try:
                # Disable stop button
                signals.set_stop_enabled_signal.emit(False)
            except Exception:
                pass

    def summarize_story(self, story_text, supplemental_text, append_thinking_callback):
        """Summarize the story to reduce token count.

        Args:
            story_text: The current story content
            supplemental_text: Combined supplemental prompts
            append_thinking_callback: Callback to append thinking text

        Returns:
            str: Summarized story text
        """
        try:
            summary_prompt = self.settings_model.summary_prompt_template

            append_thinking_callback(
                f"\nüîç Using summary template ({len(summary_prompt)} chars)\n"
            )

            if not summary_prompt.endswith("\n\n"):
                if not summary_prompt.endswith("\n"):
                    summary_prompt += "\n\n"
                else:
                    summary_prompt += "\n"

            summary_prompt += f"STORY TO SUMMARIZE:\n{story_text}\n\n"

            if supplemental_text:
                summary_prompt += f"ADDITIONAL CONTEXT (incorporate these details):\n{supplemental_text}\n\n"

            summary_prompt += "DETAILED SUMMARY:"

            model_name = getattr(self.llm, "model_name", None)

            llm_no_streaming = ChatOpenAI(
                base_url=self.llm_model.base_url,
                model=model_name if model_name else "default",
                streaming=False,
                temperature=0.3,
            )

            response = llm_no_streaming.invoke([HumanMessage(content=summary_prompt)])
            summary = (
                response.content if hasattr(response, "content") else str(response)
            )

            return summary
        except Exception as e:
            # Fallback to intelligent truncation
            context_limit = self.settings_model.context_limit
            max_chars = int(context_limit * 0.3 * 4)

            if len(story_text) > max_chars:
                truncated = story_text[-max_chars:]
                first_newline = truncated.find("\n\n")
                if first_newline > 0:
                    truncated = truncated[first_newline:].strip()

                append_thinking_callback(f"\n‚ö†Ô∏è Summarization failed: {e}\n")
                append_thinking_callback("Using automatic truncation instead...\n\n")

                return f"[TRUNCATED - Most recent content only]:\n{truncated}"
            else:
                return story_text

    def clear_chunk_cache(self):
        """Clear the cached chunked story for the current session."""
        self._cached_chunks = None
        self._cached_chunks_hash = None

    def summarize_supplemental(
        self, supp_text: str, max_tokens: int
    ) -> tuple[str, int]:
        """Summarize supplemental prompts to fit within token limit.

        Args:
            supp_text: Supplemental prompts text
            max_tokens: Maximum tokens allowed

        Returns:
            tuple: (summarized_text, estimated_tokens)
        """
        try:
            summary_prompt = (
                f"You are condensing supplemental writing prompts into a concise summary of approximately {max_tokens} tokens or less. "
                "Preserve key instructions, character details, world-building notes, and style guidelines. "
                "Combine related items and eliminate redundancy while maintaining all critical information.\n\n"
                f"SUPPLEMENTAL PROMPTS TO CONDENSE:\n{supp_text}\n\n"
                "CONDENSED VERSION:"
            )

            model_name = getattr(self.llm, "model_name", None)

            llm_no_streaming = ChatOpenAI(
                base_url=self.llm_model.base_url,
                model=model_name if model_name else "default",
                streaming=False,
                temperature=0.3,
            )

            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                response = llm_no_streaming.invoke(
                    [HumanMessage(content=summary_prompt)]
                )

            if hasattr(response, "content"):
                condensed = response.content
            elif hasattr(response, "text"):
                condensed = response.text
            else:
                condensed = str(response)

            if not isinstance(condensed, str):
                condensed = str(condensed)

            tokens = StoryModel.estimate_token_count(condensed)

            return condensed, tokens

        except Exception as e:
            # Fallback: truncate intelligently
            max_chars = max_tokens * 4
            truncated = supp_text[:max_chars]
            tokens = StoryModel.estimate_token_count(truncated)
            return f"[CONDENSED SUPPLEMENTAL]:\n{truncated}", tokens

    def summarize_system_prompt(
        self, system_prompt: str, max_tokens: int
    ) -> tuple[str, int]:
        """Summarize system prompt to fit within token limit.

        Args:
            system_prompt: System prompt text
            max_tokens: Maximum tokens allowed

        Returns:
            tuple: (summarized_text, estimated_tokens)
        """
        try:
            summary_prompt = (
                f"You are condensing a system prompt into approximately {max_tokens} tokens or less. "
                "Preserve the core instructions, personality traits, behavioral guidelines, and critical rules. "
                "Be concise but maintain all essential directives.\n\n"
                f"SYSTEM PROMPT TO CONDENSE:\n{system_prompt}\n\n"
                "CONDENSED VERSION:"
            )

            model_name = getattr(self.llm, "model_name", None)

            llm_no_streaming = ChatOpenAI(
                base_url=self.llm_model.base_url,
                model=model_name if model_name else "default",
                streaming=False,
                temperature=0.3,
            )

            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                response = llm_no_streaming.invoke(
                    [HumanMessage(content=summary_prompt)]
                )

            if hasattr(response, "content"):
                condensed = response.content
            elif hasattr(response, "text"):
                condensed = response.text
            else:
                condensed = str(response)

            if not isinstance(condensed, str):
                condensed = str(condensed)

            tokens = StoryModel.estimate_token_count(condensed)

            return condensed, tokens

        except Exception as e:
            # Fallback: truncate intelligently
            max_chars = max_tokens * 4
            truncated = system_prompt[:max_chars]
            tokens = StoryModel.estimate_token_count(truncated)
            return truncated, tokens

    def summarize_rag_context(
        self, rag_context: str, max_tokens: int
    ) -> tuple[str, int]:
        """Summarize RAG context to fit within token limit.

        Args:
            rag_context: RAG retrieved context
            max_tokens: Maximum tokens allowed

        Returns:
            tuple: (summarized_text, estimated_tokens)
        """
        try:
            summary_prompt = (
                f"You are condensing retrieved knowledge base context into approximately {max_tokens} tokens or less. "
                "Preserve key facts, relationships, and relevant information. "
                "Eliminate redundancy while maintaining context relevance.\n\n"
                f"CONTEXT TO CONDENSE:\n{rag_context}\n\n"
                "CONDENSED VERSION:"
            )

            model_name = getattr(self.llm, "model_name", None)

            llm_no_streaming = ChatOpenAI(
                base_url=self.llm_model.base_url,
                model=model_name if model_name else "default",
                streaming=False,
                temperature=0.3,
            )

            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                response = llm_no_streaming.invoke(
                    [HumanMessage(content=summary_prompt)]
                )

            if hasattr(response, "content"):
                condensed = response.content
            elif hasattr(response, "text"):
                condensed = response.text
            else:
                condensed = str(response)

            if not isinstance(condensed, str):
                condensed = str(condensed)

            tokens = StoryModel.estimate_token_count(condensed)

            return condensed, tokens

        except Exception as e:
            # Fallback: truncate intelligently
            max_chars = max_tokens * 4
            truncated = rag_context[:max_chars]
            tokens = StoryModel.estimate_token_count(truncated)
            return f"[CONDENSED RAG CONTEXT]:\n{truncated}", tokens

    def generate_notes(
        self, story_context: str, notes_prompt_template: str, on_chunk_callback=None
    ) -> tuple[str, int]:
        """Generate structured notes for the current scene using the notes prompt.

        Args:
            story_context: The current story content for context
            notes_prompt_template: Template/instructions for notes generation
            on_chunk_callback: Optional callback for streaming chunks

        Returns:
            tuple: (generated_notes, estimated_tokens)
        """
        try:
            # Build the generation prompt
            generation_prompt = (
                f"{notes_prompt_template}\n\n"
                f"STORY CONTEXT:\n{story_context}\n\n"
                "GENERATED NOTES:"
            )

            model_name = getattr(self.llm, "model_name", None)

            # Create LLM with streaming enabled if callback provided
            llm_for_notes = ChatOpenAI(
                base_url=self.llm_model.base_url,
                model=model_name if model_name else "default",
                streaming=bool(on_chunk_callback),
                temperature=0.5,  # Slightly creative but focused
            )

            generated_notes = ""
            in_thinking_block = False

            with warnings.catch_warnings():
                warnings.simplefilter("ignore")

                if on_chunk_callback:
                    # Use streaming with thinking block filtering
                    for chunk in llm_for_notes.stream(
                        [HumanMessage(content=generation_prompt)]
                    ):
                        if hasattr(chunk, "content"):
                            text = chunk.content
                        elif hasattr(chunk, "text"):
                            text = chunk.text
                        else:
                            text = str(chunk)

                        if text:
                            # Filter out thinking blocks from notes
                            if "<think>" in text:
                                before_think = text.split("<think>")[0]
                                if before_think:
                                    generated_notes += before_think
                                    on_chunk_callback(before_think)
                                in_thinking_block = True
                                text = (
                                    text.split("<think>", 1)[1]
                                    if "<think>" in text
                                    else ""
                                )

                            if "</think>" in text and in_thinking_block:
                                after_think = (
                                    text.split("</think>", 1)[1]
                                    if "</think>" in text
                                    else ""
                                )
                                in_thinking_block = False
                                text = after_think

                            if not in_thinking_block and text:
                                generated_notes += text
                                on_chunk_callback(text)
                else:
                    # Non-streaming
                    response = llm_for_notes.invoke(
                        [HumanMessage(content=generation_prompt)]
                    )
                    if hasattr(response, "content"):
                        generated_notes = response.content
                    elif hasattr(response, "text"):
                        generated_notes = response.text
                    else:
                        generated_notes = str(response)

            if not isinstance(generated_notes, str):
                generated_notes = str(generated_notes)

            tokens = StoryModel.estimate_token_count(generated_notes)

            return generated_notes, tokens

        except Exception as e:
            print(f"‚ö† Error generating notes: {e}")
            # Return empty notes on error
            return "", 0

    def summarize_chunk(
        self, chunk_text: str, append_thinking_callback
    ) -> tuple[str, int]:
        """Summarize a single chunk of story text.

        Args:
            chunk_text: The chunk text to summarize
            append_thinking_callback: Callback to append thinking text

        Returns:
            tuple: (summary_text, estimated_tokens)
        """
        try:
            summary_prompt = (
                "You are summarizing a portion of a story. Create a detailed but concise summary "
                "that captures key plot points, character development, and important details. "
                "Focus on events, decisions, and outcomes. Be specific about what happened.\n\n"
                f"TEXT TO SUMMARIZE:\n{chunk_text}\n\n"
                "DETAILED SUMMARY:"
            )

            model_name = getattr(self.llm, "model_name", None)

            llm_no_streaming = ChatOpenAI(
                base_url=self.llm_model.base_url,
                model=model_name if model_name else "default",
                streaming=False,
                temperature=0.3,
            )

            # Suppress warnings during invoke
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                response = llm_no_streaming.invoke(
                    [HumanMessage(content=summary_prompt)]
                )

            # Extract content safely
            if hasattr(response, "content"):
                summary = response.content
            elif hasattr(response, "text"):
                summary = response.text
            else:
                summary = str(response)

            # Ensure summary is a string
            if not isinstance(summary, str):
                summary = str(summary)

            # Estimate tokens
            tokens = StoryModel.estimate_token_count(summary)

            return summary, tokens

        except Exception as e:
            append_thinking_callback(f"\n‚ùå Chunk summarization failed: {e}\n")
            # Return truncated version as fallback
            max_chars = 800 * 4  # ~800 tokens
            truncated = chunk_text[:max_chars] + "..."
            tokens = StoryModel.estimate_token_count(truncated)
            return truncated, tokens

    def compress_rolling_summary(
        self,
        current_rolling_summary: str,
        new_chunk_summary: str,
        max_tokens: int,
        append_thinking_callback,
    ) -> tuple[str, int]:
        """Compress rolling summary by re-summarizing it with new chunk.

        Args:
            current_rolling_summary: Current rolling summary
            new_chunk_summary: New chunk summary to incorporate
            max_tokens: Maximum tokens for result
            append_thinking_callback: Callback to append thinking text

        Returns:
            tuple: (compressed_summary, estimated_tokens)
        """
        try:
            compression_prompt = (
                "You are compressing a story summary. Combine the current summary with new events "
                f"into a single cohesive summary of approximately {max_tokens} tokens or less. "
                "Prioritize recent events but maintain key earlier plot points. "
                "Be concise but specific.\n\n"
                f"CURRENT SUMMARY:\n{current_rolling_summary}\n\n"
                f"NEW EVENTS:\n{new_chunk_summary}\n\n"
                "COMPRESSED SUMMARY:"
            )

            model_name = getattr(self.llm, "model_name", None)

            llm_no_streaming = ChatOpenAI(
                base_url=self.llm_model.base_url,
                model=model_name if model_name else "default",
                streaming=False,
                temperature=0.3,
            )

            # Suppress warnings during invoke
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                response = llm_no_streaming.invoke(
                    [HumanMessage(content=compression_prompt)]
                )

            # Extract content safely
            if hasattr(response, "content"):
                compressed = response.content
            elif hasattr(response, "text"):
                compressed = response.text
            else:
                compressed = str(response)

            # Ensure compressed is a string
            if not isinstance(compressed, str):
                compressed = str(compressed)

            # Estimate tokens
            tokens = StoryModel.estimate_token_count(compressed)

            # If still too large, truncate intelligently
            if tokens > max_tokens:
                max_chars = max_tokens * 4
                compressed = compressed[:max_chars]
                # Try to end at sentence boundary
                for delimiter in [". ", "! ", "? "]:
                    last_delimiter = compressed.rfind(delimiter)
                    if last_delimiter > len(compressed) * 0.8:
                        compressed = compressed[: last_delimiter + 1]
                        break
                tokens = StoryModel.estimate_token_count(compressed)

            return compressed, tokens

        except Exception as e:
            append_thinking_callback(f"\n‚ùå Rolling summary compression failed: {e}\n")
            # Fallback: just use new chunk summary
            tokens = StoryModel.estimate_token_count(new_chunk_summary)
            return new_chunk_summary, tokens

    def process_story_with_summarization(
        self,
        current_story,
        max_raw_tokens,
        max_rolling_summary_tokens,
        summary_model,
        thinking_callback,
        completed_callback,
        error_callback,
        set_waiting_callback,
    ):
        """Process story with chunking and summarization in background thread.

        Args:
            current_story: Full story text
            max_raw_tokens: Maximum tokens for raw recent content
            max_rolling_summary_tokens: Maximum tokens for rolling summary
            summary_model: SummaryModel instance
            thinking_callback: Callback for thinking panel updates
            completed_callback: Callback when complete (story_for_llm, tokens)
            error_callback: Callback for errors
            set_waiting_callback: Callback to set waiting state
        """
        # Create signals object in main thread
        signals = SummarizationSignals()

        # Connect signals
        signals.thinking_signal.connect(thinking_callback, QtCore.Qt.QueuedConnection)
        signals.completed_signal.connect(completed_callback, QtCore.Qt.QueuedConnection)
        signals.error_signal.connect(error_callback, QtCore.Qt.QueuedConnection)
        signals.set_waiting_signal.connect(
            set_waiting_callback, QtCore.Qt.QueuedConnection
        )

        # Start background thread
        thread = threading.Thread(
            target=self._process_story_thread,
            args=(
                current_story,
                max_raw_tokens,
                max_rolling_summary_tokens,
                summary_model,
                signals,
            ),
            daemon=True,
        )
        thread.start()

    def _process_story_thread(
        self,
        current_story,
        max_raw_tokens,
        max_rolling_summary_tokens,
        summary_model,
        signals,
    ):
        """Background thread for story processing.

        Args:
            current_story: Full story text
            max_raw_tokens: Maximum tokens for raw content
            max_rolling_summary_tokens: Maximum tokens for rolling summary
            summary_model: SummaryModel instance
            signals: SummarizationSignals object
        """
        try:
            # Extract recent content
            raw_recent, split_pos = StoryModel.extract_recent_content(
                current_story, max_raw_tokens
            )
            raw_tokens = StoryModel.estimate_token_count(raw_recent)

            signals.thinking_signal.emit(
                f"‚úì Extracted {raw_tokens} tokens of recent content\n"
            )

            # Check if there's older content
            if split_pos > 0:
                older_content = current_story[:split_pos]
                older_tokens = StoryModel.estimate_token_count(older_content)

                signals.thinking_signal.emit(
                    f"üì¶ Processing {older_tokens} tokens of older content...\n\n"
                )

                # Chunk the older content (use session cache to avoid re-chunking)
                older_hash = hashlib.md5(older_content.encode("utf-8")).hexdigest()
                if (
                    self._cached_chunks_hash == older_hash
                    and self._cached_chunks is not None
                ):
                    chunks = self._cached_chunks
                    signals.thinking_signal.emit(
                        f"Using cached {len(chunks)} chunks from session memory\n"
                    )
                else:
                    chunks = StoryModel.chunk_text(
                        older_content, target_chunk_tokens=3000
                    )
                    # Cache for rest of session
                    self._cached_chunks = chunks
                    self._cached_chunks_hash = older_hash
                    signals.thinking_signal.emit(
                        f"Split into {len(chunks)} chunks at natural boundaries\n"
                    )

                # Process each chunk
                rolling_summary = summary_model.rolling_summary
                rolling_tokens = summary_model.rolling_summary_tokens

                for idx, (chunk_text, start, end) in enumerate(chunks):
                    chunk_tokens = StoryModel.estimate_token_count(chunk_text)

                    signals.thinking_signal.emit(
                        f"\nüîÑ Chunk {idx + 1}/{len(chunks)} ({chunk_tokens} tokens)...\n"
                    )

                    # Summarize this chunk
                    def thinking_cb(text):
                        signals.thinking_signal.emit(text)

                    chunk_summary, summary_tokens = self.summarize_chunk(
                        chunk_text, thinking_cb
                    )

                    signals.thinking_signal.emit(
                        f"  ‚Üí Summarized to {summary_tokens} tokens\n"
                    )

                    # Check if we need to compress rolling summary
                    combined_tokens = rolling_tokens + summary_tokens

                    if combined_tokens > max_rolling_summary_tokens and rolling_summary:
                        signals.thinking_signal.emit(
                            f"  ‚öôÔ∏è Compressing rolling summary ({combined_tokens} ‚Üí {max_rolling_summary_tokens})...\n"
                        )

                        rolling_summary, rolling_tokens = self.compress_rolling_summary(
                            rolling_summary,
                            chunk_summary,
                            max_rolling_summary_tokens,
                            thinking_cb,
                        )

                        signals.thinking_signal.emit(
                            f"  ‚úì Compressed to {rolling_tokens} tokens\n"
                        )
                    else:
                        # Just append to rolling summary
                        if rolling_summary:
                            rolling_summary = f"{rolling_summary}\n\n{chunk_summary}"
                        else:
                            rolling_summary = chunk_summary
                        rolling_tokens = StoryModel.estimate_token_count(
                            rolling_summary
                        )

                # Update the summary model
                summary_model.update_rolling_summary(rolling_summary, rolling_tokens)

                signals.thinking_signal.emit(f"\n{'=' * 60}\n")
                signals.thinking_signal.emit(
                    f"‚úÖ Rolling summary: {rolling_tokens} tokens\n"
                )
                signals.thinking_signal.emit(
                    f"‚úÖ Recent content: {raw_tokens} tokens\n"
                )
                signals.thinking_signal.emit(
                    f"‚úÖ Total story context: {rolling_tokens + raw_tokens} tokens\n"
                )
                signals.thinking_signal.emit(f"{'=' * 60}\n\n")

                # Build story context for LLM
                story_for_llm, story_context_tokens = summary_model.get_context_for_llm(
                    raw_recent, raw_tokens
                )
            else:
                # No older content
                story_for_llm = raw_recent
                story_context_tokens = raw_tokens

            # Signal completion
            signals.completed_signal.emit(story_for_llm, story_context_tokens)
            signals.set_waiting_signal.emit(False)

        except Exception as e:
            signals.error_signal.emit(str(e))
            signals.set_waiting_signal.emit(False)

    def generate_story_chunk(
        self,
        query,
        system_prompt,
        paragraph_limit,
        append_text_callback,
        append_thinking_callback,
        render_markdown_callback,
        set_waiting_callback,
        set_stop_enabled_callback,
    ):
        """Generate a story chunk with a paragraph limit.

        Args:
            query: The query to send to the LLM
            system_prompt: Optional system prompt
            paragraph_limit: Maximum number of paragraphs to generate
            append_text_callback: Callback to append text to view
            append_thinking_callback: Callback to append thinking text to view
            render_markdown_callback: Callback to render markdown in view
            set_waiting_callback: Callback to set waiting state
            set_stop_enabled_callback: Callback to enable/disable stop button
        """
        # Create signals object in main thread
        signals = StreamingSignals()

        # Connect signals with Qt.QueuedConnection to ensure execution on main thread
        signals.text_signal.connect(append_text_callback, QtCore.Qt.QueuedConnection)
        signals.thinking_signal.connect(
            append_thinking_callback, QtCore.Qt.QueuedConnection
        )
        signals.render_markdown_signal.connect(
            render_markdown_callback, QtCore.Qt.QueuedConnection
        )
        signals.set_waiting_signal.connect(
            set_waiting_callback, QtCore.Qt.QueuedConnection
        )
        signals.set_stop_enabled_signal.connect(
            set_stop_enabled_callback, QtCore.Qt.QueuedConnection
        )

        thread = threading.Thread(
            target=self._chunk_generation_thread,
            args=(query, system_prompt, paragraph_limit, signals),
            daemon=True,
        )
        thread.start()

    def _chunk_generation_thread(self, query, system_prompt, paragraph_limit, signals):
        """Thread function for chunk generation with paragraph limit.

        Args:
            query: The query to send to the LLM
            system_prompt: Optional system prompt
            paragraph_limit: Maximum number of paragraphs to generate
            signals: StreamingSignals object created in main thread
        """
        try:
            streaming_handler = QtStreamingCallbackHandler(
                signals,
                lambda: self.llm_model.stop_generation,
                paragraph_limit=paragraph_limit,
            )

            if system_prompt:
                messages = [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=query),
                ]
                self.llm.invoke(messages, config={"callbacks": [streaming_handler]})
            else:
                self.llm.invoke(query, config={"callbacks": [streaming_handler]})

            # Check if stop was requested while we were generating
            if self.llm_model.stop_generation:
                signals.text_signal.emit("\n[Generation stopped by user]\n")
                signals.thinking_signal.emit(
                    "\n‚èπÔ∏è Stop requested - generation halted.\n"
                )
            else:
                # Use signal to trigger markdown rendering on UI thread (only if not stopped)
                signals.render_markdown_signal.emit()

        except KeyboardInterrupt as e:
            # Check if it was user-initiated stop or paragraph limit
            error_msg = str(e)
            if "stopped by user" in error_msg.lower():
                signals.text_signal.emit("\n[Generation stopped by user]\n")
                signals.thinking_signal.emit(
                    "\n‚èπÔ∏è Stop requested - generation halted.\n"
                )
            elif "Paragraph limit" in error_msg:
                # Paragraph limit reached - this is normal, no error message needed
                pass
            else:
                signals.thinking_signal.emit(
                    f"\n‚èπÔ∏è Generation interrupted: {error_msg}\n"
                )
        except Exception as e:
            # Use signal to append error text on UI thread
            signals.text_signal.emit(f"\n[error] {e}\n")
        finally:
            try:
                # Use signal to hide progress bar on UI thread
                signals.set_waiting_signal.emit(False)
            except Exception:
                pass
